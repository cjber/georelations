<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>src.common.utils API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.common.utils</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np
import torch
from collections import Counter
from ekphrasis.classes.tokenizer import SocialTokenizer
from ekphrasis.dicts.emoticons import emoticons
from torch import Tensor
from typing import Union

tdict = dict[str, Tensor]


class Const:
    MODEL_NAME = &#34;roberta-base&#34;
    MAX_TOKEN_LEN = 64
    SPECIAL_TOKENS = [
        &#34;&lt;head&gt;&#34;,
        &#34;&lt;/head&gt;&#34;,
        &#34;&lt;tail&gt;&#34;,
        &#34;&lt;/tail&gt;&#34;,
        &#34;&lt;url&gt;&#34;,
        &#34;&lt;user&gt;&#34;,
        &#34;&lt;date&gt;&#34;,
        &#34;&lt;number&gt;&#34;,
        &#34;&lt;money&gt;&#34;,
        &#34;&lt;email&gt;&#34;,
        &#34;&lt;percent&gt;&#34;,
        &#34;&lt;phone&gt;&#34;,
        &#34;&lt;time&gt;&#34;,
        &#34;&lt;hashtag&gt;&#34;,
        &#34;&lt;/hashtag&gt;&#34;,
    ]
    NORMALIZE = [
        &#34;url&#34;,
        &#34;email&#34;,
        &#34;percent&#34;,
        &#34;money&#34;,
        &#34;phone&#34;,
        &#34;user&#34;,
        &#34;time&#34;,
        &#34;url&#34;,
        &#34;date&#34;,
        &#34;number&#34;,
    ]

    TEXT_PROCESSOR_ARGS = dict(
        normalize=NORMALIZE,
        annotate={&#34;hashtag&#34;},
        fix_html=True,
        segmenter=&#34;twitter&#34;,
        corrector=&#34;twitter&#34;,
        unpack_hashtags=True,
        unpack_contractions=True,
        spell_correct_elong=False,
        tokenizer=SocialTokenizer(lowercase=True).tokenize,
        dicts=[emoticons],
    )


class Label:
    def __init__(self, name: str):
        &#34;&#34;&#34;
        Class used to create labels based on task.

        Parameters
        ----------
        name : str
            Name of task, either GER or REL.
        &#34;&#34;&#34;
        self.name = name
        assert self.name in {&#34;GER&#34;, &#34;REL&#34;}, &#34;Type must be either GER or REL&#34;

        if self.name == &#34;GER&#34;:
            self.labels: dict[str, int] = {
                &#34;O&#34;: 0,
                &#34;B-location&#34;: 1,
                &#34;I-location&#34;: 2,
                &#34;L-location&#34;: 3,
                &#34;U-location&#34;: 4,
            }
        elif self.name == &#34;REL&#34;:
            self.labels: dict[str, int] = {
                &#34;contains&#34;: 0,
                &#34;none&#34;: 1,
            }

        self.idx: dict[int, str] = {v: k for k, v in self.labels.items()}
        self.count: int = len(self.labels)


def encode_labels(
    tokens: list[str],
    labels: list[int],
    tokenizer,
    max_token_len: int,
) -&gt; tuple[dict[str, Tensor], Tensor]:
    &#34;&#34;&#34;
    Encode list of tokens and labels into subwords.

    Input should be list of string tokens and integer labels of the same length,
    tokenizer should be pre-trained and have `add_prefix_space=True`.
    Output gives the encoding with token `input_ids` and `attention_mask`,
    `labels_encoded` gives the original label sequence but with -100 added where
    words have been split into sub words, or there is padding.

    Parameters
    ----------
    tokens : list[str]
        Pre-tokenized words in sequence
    labels : list[int]
        Label IDs for each token in sequence
    tokenizer : transformers.PretrainedTokenizer
        HuggingFace tokenizer
    max_token_len : int
        Cutoff length for number of tokens

    Returns
    -------
    tuple[dict, np.ndarray]:
        Encoded subwords and labels

    Example
    -------

    &gt;&gt;&gt; from transformers import AutoTokenizer
    &gt;&gt;&gt; tokens = [&#39;Testing&#39;, &#39;this&#39;, &#39;for&#39;, &#39;doctest&#39;, &#39;.&#39;]
    &gt;&gt;&gt; labels = [1, 0, 0, 0, 0]
    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(
    ...     &#39;roberta-base&#39;, add_prefix_space=True
    ... )
    &gt;&gt;&gt; encoding, labels_encoded = encode_labels(tokens, labels, tokenizer, 32)

    &gt;&gt;&gt; len(encoding[&#39;input_ids&#39;].flatten()) == len(labels_encoded)
    True

    &gt;&gt;&gt; labels_encoded.tolist()[:6]
    [-100, 1, 0, 0, 0, -100]
    &#34;&#34;&#34;
    encoding = tokenizer(
        tokens,
        is_split_into_words=True,
        return_attention_mask=True,
        return_offsets_mapping=True,
        max_length=max_token_len,
        padding=&#34;max_length&#34;,
        truncation=True,
        return_tensors=&#34;pt&#34;,
    )
    offset = np.array(encoding.offset_mapping[0])
    doc_enc_labels = np.ones(max_token_len, dtype=int) * -100  # type: ignore
    offsets_array = (offset[:, 0] == 1) &amp; (offset[:, 1] != 1)
    if sum(offsets_array) &lt; len(labels):
        doc_enc_labels[offsets_array] = labels[: sum(offsets_array)]
    else:
        doc_enc_labels[offsets_array] = labels

    encoded_labels = torch.LongTensor(doc_enc_labels)
    return encoding, encoded_labels


def combine_subwords(tokens: list[str], tags: list[int]) -&gt; tuple[list[str], list[str]]:
    &#34;&#34;&#34;
    Combines subwords and their tags into normal words with special chars removed.

    Parameters
    ----------
    tokens : list[str]
        Subword tokens.
    tags : list[int]
        Token tags of same length.

    Returns
    -------
    tuple[list[str], list[str]]:
        Combined tokens and tags.

    Example
    -------

    &gt;&gt;&gt; tokens = [&#39;ĠVery&#39;, &#39;long&#39;, &#39;word&#39;, &#39;Ġfor&#39;, &#39;Ġdoct&#39;, &#39;est&#39;, &#39;Ġ.&#39;]
    &gt;&gt;&gt; tags = [4, -100, -100, 0, 4, -100, 0]
    &gt;&gt;&gt; tokens, tags = combine_subwords(tokens, tags)

    &gt;&gt;&gt; tokens
    [&#39;Verylongword&#39;, &#39;for&#39;, &#39;doctest&#39;, &#39;.&#39;]
    &gt;&gt;&gt; len(tags) == len(tokens)
    True
    &#34;&#34;&#34;

    idx = [
        idx for idx, token in enumerate(tokens) if token not in [&#34;&lt;s&gt;&#34;, &#34;&lt;pad&gt;&#34;, &#34;&lt;/s&gt;&#34;]
    ]

    tokens = [tokens[i] for i in idx]
    tags = [tags[i] for i in idx]

    for idx, _ in enumerate(tokens):
        idx += 1
        if not tokens[-idx + 1].startswith(&#34;Ġ&#34;):
            tokens[-idx] = tokens[-idx] + tokens[-idx + 1]
    subwords = [i for i, _ in enumerate(tokens) if tokens[i].startswith(&#34;Ġ&#34;)]

    tags = [tags[i] for i in subwords]
    tokens = [tokens[i][1:] for i in subwords]
    tags_str: list[str] = [Label(&#34;GER&#34;).idx[i] for i in tags]
    return tokens, tags_str


def combine_biluo(tokens: list[str], tags: list[str]) -&gt; tuple[list[str], list[str]]:
    &#34;&#34;&#34;
    Combines multi-token BILUO tags into single entities.

    Parameters
    ----------
    tokens : list[str]
        Input tokenized string.
    tags : list[str]
        Tags corresponding with each token with BILUO format.

    Returns
    -------
    tuple[list[str], list[str]]:
        Tokens and tags with BILUO removed.

    Example
    -------

    &gt;&gt;&gt; tokens = [&#39;New&#39;, &#39;York&#39;, &#39;City&#39;, &#39;is&#39;, &#39;big&#39;, &#39;.&#39;]
    &gt;&gt;&gt; tags = [&#39;B-PLACE&#39;, &#39;I-PLACE&#39;, &#39;L-City&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;]
    &gt;&gt;&gt; tokens, tags = combine_biluo(tokens, tags)

    &gt;&gt;&gt; tokens
    [&#39;New York City&#39;, &#39;is&#39;, &#39;big&#39;, &#39;.&#39;]
    &gt;&gt;&gt; tags
    [&#39;PLACE&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;]
    &#34;&#34;&#34;
    tokens_biluo = tokens.copy()
    tags_biluo = tags.copy()

    for idx, tag in enumerate(tags_biluo):
        if idx + 1 &lt; len(tags_biluo) and tag[0] == &#34;B&#34;:
            i = 1
            while tags_biluo[idx + i][0] not in [&#34;B&#34;, &#34;O&#34;]:
                tokens_biluo[idx] = tokens_biluo[idx] + &#34; &#34; + tokens_biluo[idx + i]
                i += 1
                if idx + i == len(tokens_biluo):
                    break

    zipped = [
        (token, tag)
        for (token, tag) in zip(tokens_biluo, tags_biluo)
        if tag[0] not in [&#34;I&#34;, &#34;L&#34;]
    ]
    if list(zipped):
        tokens_biluo, tags_biluo = zip(*zipped)
        tags_biluo = [tag[2:] if tag != &#34;O&#34; else tag for tag in tags_biluo]
        return list(tokens_biluo), tags_biluo
    else:
        return [], []


def ents_to_relations(tokens: list[str], tags: list[str]) -&gt; Union[list[str], None]:
    &#34;&#34;&#34;
    Convert a list of tokens and tags into relations using the head and tail format
    used by the NYT corpus.

    Parameters
    ----------
    tokens : list[str]
        List of tokens.
    tags : list[str]
        List of tags.

    Returns
    -------
    Union[list[str], None]
        List of strings with head and tail annotations.

    Example
    -------

    &gt;&gt;&gt; tokens = [&#39;New York&#39;, &#39;is&#39;, &#39;in&#39;, &#39;New York&#39;, &#39;,&#39;, &#39;America&#39;]
    &gt;&gt;&gt; tags = [&#39;PLACE&#39;, &#39;O&#39;, &#39;O&#39;, &#39;PLACE&#39;, &#39;O&#39;, &#39;PLACE&#39;]

    &gt;&gt;&gt; ents_to_relations(tokens, tags)[0]
    &#39;&lt;head&gt; New York &lt;/head&gt; is in &lt;tail&gt; New York &lt;/tail&gt; , America&#39;
    &gt;&gt;&gt; ents_to_relations(tokens, tags)[1]
    &#39;&lt;head&gt; New York &lt;/head&gt; is in New York , &lt;tail&gt; America &lt;/tail&gt;&#39;
    &#34;&#34;&#34;

    # check if at least two entities
    if Counter(tags)[&#34;O&#34;] &gt; len(tags) - 2:
        return

    loc_idxs = [idx for idx, tag in enumerate(tags) if tag != &#34;O&#34;]
    sequence_list = []
    for i in loc_idxs:
        for j in loc_idxs:
            if i != j:
                tokens_copy = tokens.copy()
                tokens_copy[i] = &#34;&lt;head&gt; &#34; + tokens_copy[i] + &#34; &lt;/head&gt;&#34;
                tokens_copy[j] = &#34;&lt;tail&gt; &#34; + tokens_copy[j] + &#34; &lt;/tail&gt;&#34;
                sequence_list.append(&#34; &#34;.join(tokens_copy))
    return sequence_list


def convert_input(item: dict, max_seq_len: int, tokenizer) -&gt; Union[dict, None]:
    if &#34;relation&#34; in item:
        tokens_a = tokenizer.tokenize(
            item[&#34;sentence&#34;],
            # max_length=max_seq_len,
            truncation=True,
            return_tensors=&#34;pt&#34;,
        )
        label_id = int(item[&#34;relation&#34;])
    else:
        tokens_a = tokenizer.tokenize(
            item[&#34;sentence&#34;],
            # max_length=max_seq_len,
            truncation=True,
            return_tensors=&#34;pt&#34;,
        )
        label_id = None

    e11_p = tokens_a.index(&#34;&lt;head&gt;&#34;)  # the start position of entity1
    e12_p = tokens_a.index(&#34;&lt;/head&gt;&#34;)  # the end position of entity1
    e21_p = tokens_a.index(&#34;&lt;tail&gt;&#34;)  # the start position of entity2
    e22_p = tokens_a.index(&#34;&lt;/tail&gt;&#34;)  # the end position of entity2

    tokens_a = [&#34;&lt;s&gt;&#34;] + tokens_a

    # Account for [CLS] and [SEP] with &#34;- 2&#34; and with &#34;- 3&#34; for RoBERTa.
    if len(tokens_a) &gt; max_seq_len - 1:
        tokens_a = tokens_a[: (max_seq_len - 1)]

    if all(x in tokens_a for x in [&#34;&lt;head&gt;&#34;, &#34;&lt;/head&gt;&#34;, &#34;&lt;tail&gt;&#34;, &#34;&lt;/tail&gt;&#34;]):
        input_ids = tokenizer.convert_tokens_to_ids(tokens_a)

        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.
        attention_mask = [1] * len(input_ids)

        # Zero-pad up to the sequence length.
        padding_length = max_seq_len - len(input_ids)
        input_ids = input_ids + ([1] * padding_length)
        attention_mask = attention_mask + ([0] * padding_length)

        # e1 mask, e2 mask
        e1_mask = [0] * len(attention_mask)
        e2_mask = [0] * len(attention_mask)

        if len(input_ids):
            for i in range(e11_p, e12_p + 1):
                e1_mask[i] = 1
            for i in range(e21_p, e22_p + 1):
                e2_mask[i] = 1

            assert (
                len(input_ids) == max_seq_len
            ), f&#34;Error with input length {len(input_ids)} vs {max_seq_len}&#34;
            assert (
                len(attention_mask) == max_seq_len
            ), f&#34;Error with attention mask length {len(attention_mask)} vs {max_seq_len}&#34;

            return {
                &#34;input_ids&#34;: torch.tensor(input_ids, dtype=torch.long),
                &#34;attention_mask&#34;: torch.tensor(attention_mask, dtype=torch.long),
                &#34;labels&#34;: torch.tensor(label_id, dtype=torch.long)
                if label_id is not None
                else None,
                &#34;e1_mask&#34;: torch.tensor(e1_mask, dtype=torch.long),
                &#34;e2_mask&#34;: torch.tensor(e2_mask, dtype=torch.long),
            }


if __name__ == &#34;__main__&#34;:
    import doctest

    doctest.testmod()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.common.utils.combine_biluo"><code class="name flex">
<span>def <span class="ident">combine_biluo</span></span>(<span>tokens: list, tags: list) ‑> tuple</span>
</code></dt>
<dd>
<div class="desc"><p>Combines multi-token BILUO tags into single entities.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>tokens</code></strong> :&ensp;<code>list[str]</code></dt>
<dd>Input tokenized string.</dd>
<dt><strong><code>tags</code></strong> :&ensp;<code>list[str]</code></dt>
<dd>Tags corresponding with each token with BILUO format.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple[list[str], list[str]]:</code></dt>
<dd>Tokens and tags with BILUO removed.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; tokens = ['New', 'York', 'City', 'is', 'big', '.']
&gt;&gt;&gt; tags = ['B-PLACE', 'I-PLACE', 'L-City', 'O', 'O', 'O']
&gt;&gt;&gt; tokens, tags = combine_biluo(tokens, tags)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; tokens
['New York City', 'is', 'big', '.']
&gt;&gt;&gt; tags
['PLACE', 'O', 'O', 'O']
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def combine_biluo(tokens: list[str], tags: list[str]) -&gt; tuple[list[str], list[str]]:
    &#34;&#34;&#34;
    Combines multi-token BILUO tags into single entities.

    Parameters
    ----------
    tokens : list[str]
        Input tokenized string.
    tags : list[str]
        Tags corresponding with each token with BILUO format.

    Returns
    -------
    tuple[list[str], list[str]]:
        Tokens and tags with BILUO removed.

    Example
    -------

    &gt;&gt;&gt; tokens = [&#39;New&#39;, &#39;York&#39;, &#39;City&#39;, &#39;is&#39;, &#39;big&#39;, &#39;.&#39;]
    &gt;&gt;&gt; tags = [&#39;B-PLACE&#39;, &#39;I-PLACE&#39;, &#39;L-City&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;]
    &gt;&gt;&gt; tokens, tags = combine_biluo(tokens, tags)

    &gt;&gt;&gt; tokens
    [&#39;New York City&#39;, &#39;is&#39;, &#39;big&#39;, &#39;.&#39;]
    &gt;&gt;&gt; tags
    [&#39;PLACE&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;]
    &#34;&#34;&#34;
    tokens_biluo = tokens.copy()
    tags_biluo = tags.copy()

    for idx, tag in enumerate(tags_biluo):
        if idx + 1 &lt; len(tags_biluo) and tag[0] == &#34;B&#34;:
            i = 1
            while tags_biluo[idx + i][0] not in [&#34;B&#34;, &#34;O&#34;]:
                tokens_biluo[idx] = tokens_biluo[idx] + &#34; &#34; + tokens_biluo[idx + i]
                i += 1
                if idx + i == len(tokens_biluo):
                    break

    zipped = [
        (token, tag)
        for (token, tag) in zip(tokens_biluo, tags_biluo)
        if tag[0] not in [&#34;I&#34;, &#34;L&#34;]
    ]
    if list(zipped):
        tokens_biluo, tags_biluo = zip(*zipped)
        tags_biluo = [tag[2:] if tag != &#34;O&#34; else tag for tag in tags_biluo]
        return list(tokens_biluo), tags_biluo
    else:
        return [], []</code></pre>
</details>
</dd>
<dt id="src.common.utils.combine_subwords"><code class="name flex">
<span>def <span class="ident">combine_subwords</span></span>(<span>tokens: list, tags: list) ‑> tuple</span>
</code></dt>
<dd>
<div class="desc"><p>Combines subwords and their tags into normal words with special chars removed.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>tokens</code></strong> :&ensp;<code>list[str]</code></dt>
<dd>Subword tokens.</dd>
<dt><strong><code>tags</code></strong> :&ensp;<code>list[int]</code></dt>
<dd>Token tags of same length.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple[list[str], list[str]]:</code></dt>
<dd>Combined tokens and tags.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; tokens = ['ĠVery', 'long', 'word', 'Ġfor', 'Ġdoct', 'est', 'Ġ.']
&gt;&gt;&gt; tags = [4, -100, -100, 0, 4, -100, 0]
&gt;&gt;&gt; tokens, tags = combine_subwords(tokens, tags)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; tokens
['Verylongword', 'for', 'doctest', '.']
&gt;&gt;&gt; len(tags) == len(tokens)
True
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def combine_subwords(tokens: list[str], tags: list[int]) -&gt; tuple[list[str], list[str]]:
    &#34;&#34;&#34;
    Combines subwords and their tags into normal words with special chars removed.

    Parameters
    ----------
    tokens : list[str]
        Subword tokens.
    tags : list[int]
        Token tags of same length.

    Returns
    -------
    tuple[list[str], list[str]]:
        Combined tokens and tags.

    Example
    -------

    &gt;&gt;&gt; tokens = [&#39;ĠVery&#39;, &#39;long&#39;, &#39;word&#39;, &#39;Ġfor&#39;, &#39;Ġdoct&#39;, &#39;est&#39;, &#39;Ġ.&#39;]
    &gt;&gt;&gt; tags = [4, -100, -100, 0, 4, -100, 0]
    &gt;&gt;&gt; tokens, tags = combine_subwords(tokens, tags)

    &gt;&gt;&gt; tokens
    [&#39;Verylongword&#39;, &#39;for&#39;, &#39;doctest&#39;, &#39;.&#39;]
    &gt;&gt;&gt; len(tags) == len(tokens)
    True
    &#34;&#34;&#34;

    idx = [
        idx for idx, token in enumerate(tokens) if token not in [&#34;&lt;s&gt;&#34;, &#34;&lt;pad&gt;&#34;, &#34;&lt;/s&gt;&#34;]
    ]

    tokens = [tokens[i] for i in idx]
    tags = [tags[i] for i in idx]

    for idx, _ in enumerate(tokens):
        idx += 1
        if not tokens[-idx + 1].startswith(&#34;Ġ&#34;):
            tokens[-idx] = tokens[-idx] + tokens[-idx + 1]
    subwords = [i for i, _ in enumerate(tokens) if tokens[i].startswith(&#34;Ġ&#34;)]

    tags = [tags[i] for i in subwords]
    tokens = [tokens[i][1:] for i in subwords]
    tags_str: list[str] = [Label(&#34;GER&#34;).idx[i] for i in tags]
    return tokens, tags_str</code></pre>
</details>
</dd>
<dt id="src.common.utils.convert_input"><code class="name flex">
<span>def <span class="ident">convert_input</span></span>(<span>item: dict, max_seq_len: int, tokenizer) ‑> Optional[dict]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convert_input(item: dict, max_seq_len: int, tokenizer) -&gt; Union[dict, None]:
    if &#34;relation&#34; in item:
        tokens_a = tokenizer.tokenize(
            item[&#34;sentence&#34;],
            # max_length=max_seq_len,
            truncation=True,
            return_tensors=&#34;pt&#34;,
        )
        label_id = int(item[&#34;relation&#34;])
    else:
        tokens_a = tokenizer.tokenize(
            item[&#34;sentence&#34;],
            # max_length=max_seq_len,
            truncation=True,
            return_tensors=&#34;pt&#34;,
        )
        label_id = None

    e11_p = tokens_a.index(&#34;&lt;head&gt;&#34;)  # the start position of entity1
    e12_p = tokens_a.index(&#34;&lt;/head&gt;&#34;)  # the end position of entity1
    e21_p = tokens_a.index(&#34;&lt;tail&gt;&#34;)  # the start position of entity2
    e22_p = tokens_a.index(&#34;&lt;/tail&gt;&#34;)  # the end position of entity2

    tokens_a = [&#34;&lt;s&gt;&#34;] + tokens_a

    # Account for [CLS] and [SEP] with &#34;- 2&#34; and with &#34;- 3&#34; for RoBERTa.
    if len(tokens_a) &gt; max_seq_len - 1:
        tokens_a = tokens_a[: (max_seq_len - 1)]

    if all(x in tokens_a for x in [&#34;&lt;head&gt;&#34;, &#34;&lt;/head&gt;&#34;, &#34;&lt;tail&gt;&#34;, &#34;&lt;/tail&gt;&#34;]):
        input_ids = tokenizer.convert_tokens_to_ids(tokens_a)

        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.
        attention_mask = [1] * len(input_ids)

        # Zero-pad up to the sequence length.
        padding_length = max_seq_len - len(input_ids)
        input_ids = input_ids + ([1] * padding_length)
        attention_mask = attention_mask + ([0] * padding_length)

        # e1 mask, e2 mask
        e1_mask = [0] * len(attention_mask)
        e2_mask = [0] * len(attention_mask)

        if len(input_ids):
            for i in range(e11_p, e12_p + 1):
                e1_mask[i] = 1
            for i in range(e21_p, e22_p + 1):
                e2_mask[i] = 1

            assert (
                len(input_ids) == max_seq_len
            ), f&#34;Error with input length {len(input_ids)} vs {max_seq_len}&#34;
            assert (
                len(attention_mask) == max_seq_len
            ), f&#34;Error with attention mask length {len(attention_mask)} vs {max_seq_len}&#34;

            return {
                &#34;input_ids&#34;: torch.tensor(input_ids, dtype=torch.long),
                &#34;attention_mask&#34;: torch.tensor(attention_mask, dtype=torch.long),
                &#34;labels&#34;: torch.tensor(label_id, dtype=torch.long)
                if label_id is not None
                else None,
                &#34;e1_mask&#34;: torch.tensor(e1_mask, dtype=torch.long),
                &#34;e2_mask&#34;: torch.tensor(e2_mask, dtype=torch.long),
            }</code></pre>
</details>
</dd>
<dt id="src.common.utils.encode_labels"><code class="name flex">
<span>def <span class="ident">encode_labels</span></span>(<span>tokens: list, labels: list, tokenizer, max_token_len: int) ‑> tuple</span>
</code></dt>
<dd>
<div class="desc"><p>Encode list of tokens and labels into subwords.</p>
<p>Input should be list of string tokens and integer labels of the same length,
tokenizer should be pre-trained and have <code>add_prefix_space=True</code>.
Output gives the encoding with token <code>input_ids</code> and <code>attention_mask</code>,
<code>labels_encoded</code> gives the original label sequence but with -100 added where
words have been split into sub words, or there is padding.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>tokens</code></strong> :&ensp;<code>list[str]</code></dt>
<dd>Pre-tokenized words in sequence</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>list[int]</code></dt>
<dd>Label IDs for each token in sequence</dd>
<dt><strong><code>tokenizer</code></strong> :&ensp;<code>transformers.PretrainedTokenizer</code></dt>
<dd>HuggingFace tokenizer</dd>
<dt><strong><code>max_token_len</code></strong> :&ensp;<code>int</code></dt>
<dd>Cutoff length for number of tokens</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple[dict, np.ndarray]:</code></dt>
<dd>Encoded subwords and labels</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from transformers import AutoTokenizer
&gt;&gt;&gt; tokens = ['Testing', 'this', 'for', 'doctest', '.']
&gt;&gt;&gt; labels = [1, 0, 0, 0, 0]
&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(
...     'roberta-base', add_prefix_space=True
... )
&gt;&gt;&gt; encoding, labels_encoded = encode_labels(tokens, labels, tokenizer, 32)
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; len(encoding['input_ids'].flatten()) == len(labels_encoded)
True
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; labels_encoded.tolist()[:6]
[-100, 1, 0, 0, 0, -100]
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def encode_labels(
    tokens: list[str],
    labels: list[int],
    tokenizer,
    max_token_len: int,
) -&gt; tuple[dict[str, Tensor], Tensor]:
    &#34;&#34;&#34;
    Encode list of tokens and labels into subwords.

    Input should be list of string tokens and integer labels of the same length,
    tokenizer should be pre-trained and have `add_prefix_space=True`.
    Output gives the encoding with token `input_ids` and `attention_mask`,
    `labels_encoded` gives the original label sequence but with -100 added where
    words have been split into sub words, or there is padding.

    Parameters
    ----------
    tokens : list[str]
        Pre-tokenized words in sequence
    labels : list[int]
        Label IDs for each token in sequence
    tokenizer : transformers.PretrainedTokenizer
        HuggingFace tokenizer
    max_token_len : int
        Cutoff length for number of tokens

    Returns
    -------
    tuple[dict, np.ndarray]:
        Encoded subwords and labels

    Example
    -------

    &gt;&gt;&gt; from transformers import AutoTokenizer
    &gt;&gt;&gt; tokens = [&#39;Testing&#39;, &#39;this&#39;, &#39;for&#39;, &#39;doctest&#39;, &#39;.&#39;]
    &gt;&gt;&gt; labels = [1, 0, 0, 0, 0]
    &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(
    ...     &#39;roberta-base&#39;, add_prefix_space=True
    ... )
    &gt;&gt;&gt; encoding, labels_encoded = encode_labels(tokens, labels, tokenizer, 32)

    &gt;&gt;&gt; len(encoding[&#39;input_ids&#39;].flatten()) == len(labels_encoded)
    True

    &gt;&gt;&gt; labels_encoded.tolist()[:6]
    [-100, 1, 0, 0, 0, -100]
    &#34;&#34;&#34;
    encoding = tokenizer(
        tokens,
        is_split_into_words=True,
        return_attention_mask=True,
        return_offsets_mapping=True,
        max_length=max_token_len,
        padding=&#34;max_length&#34;,
        truncation=True,
        return_tensors=&#34;pt&#34;,
    )
    offset = np.array(encoding.offset_mapping[0])
    doc_enc_labels = np.ones(max_token_len, dtype=int) * -100  # type: ignore
    offsets_array = (offset[:, 0] == 1) &amp; (offset[:, 1] != 1)
    if sum(offsets_array) &lt; len(labels):
        doc_enc_labels[offsets_array] = labels[: sum(offsets_array)]
    else:
        doc_enc_labels[offsets_array] = labels

    encoded_labels = torch.LongTensor(doc_enc_labels)
    return encoding, encoded_labels</code></pre>
</details>
</dd>
<dt id="src.common.utils.ents_to_relations"><code class="name flex">
<span>def <span class="ident">ents_to_relations</span></span>(<span>tokens: list, tags: list) ‑> Optional[list]</span>
</code></dt>
<dd>
<div class="desc"><p>Convert a list of tokens and tags into relations using the head and tail format
used by the NYT corpus.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>tokens</code></strong> :&ensp;<code>list[str]</code></dt>
<dd>List of tokens.</dd>
<dt><strong><code>tags</code></strong> :&ensp;<code>list[str]</code></dt>
<dd>List of tags.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Union[list[str], None]</code></dt>
<dd>List of strings with head and tail annotations.</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; tokens = ['New York', 'is', 'in', 'New York', ',', 'America']
&gt;&gt;&gt; tags = ['PLACE', 'O', 'O', 'PLACE', 'O', 'PLACE']
</code></pre>
<pre><code class="language-python-repl">&gt;&gt;&gt; ents_to_relations(tokens, tags)[0]
'&lt;head&gt; New York &lt;/head&gt; is in &lt;tail&gt; New York &lt;/tail&gt; , America'
&gt;&gt;&gt; ents_to_relations(tokens, tags)[1]
'&lt;head&gt; New York &lt;/head&gt; is in New York , &lt;tail&gt; America &lt;/tail&gt;'
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ents_to_relations(tokens: list[str], tags: list[str]) -&gt; Union[list[str], None]:
    &#34;&#34;&#34;
    Convert a list of tokens and tags into relations using the head and tail format
    used by the NYT corpus.

    Parameters
    ----------
    tokens : list[str]
        List of tokens.
    tags : list[str]
        List of tags.

    Returns
    -------
    Union[list[str], None]
        List of strings with head and tail annotations.

    Example
    -------

    &gt;&gt;&gt; tokens = [&#39;New York&#39;, &#39;is&#39;, &#39;in&#39;, &#39;New York&#39;, &#39;,&#39;, &#39;America&#39;]
    &gt;&gt;&gt; tags = [&#39;PLACE&#39;, &#39;O&#39;, &#39;O&#39;, &#39;PLACE&#39;, &#39;O&#39;, &#39;PLACE&#39;]

    &gt;&gt;&gt; ents_to_relations(tokens, tags)[0]
    &#39;&lt;head&gt; New York &lt;/head&gt; is in &lt;tail&gt; New York &lt;/tail&gt; , America&#39;
    &gt;&gt;&gt; ents_to_relations(tokens, tags)[1]
    &#39;&lt;head&gt; New York &lt;/head&gt; is in New York , &lt;tail&gt; America &lt;/tail&gt;&#39;
    &#34;&#34;&#34;

    # check if at least two entities
    if Counter(tags)[&#34;O&#34;] &gt; len(tags) - 2:
        return

    loc_idxs = [idx for idx, tag in enumerate(tags) if tag != &#34;O&#34;]
    sequence_list = []
    for i in loc_idxs:
        for j in loc_idxs:
            if i != j:
                tokens_copy = tokens.copy()
                tokens_copy[i] = &#34;&lt;head&gt; &#34; + tokens_copy[i] + &#34; &lt;/head&gt;&#34;
                tokens_copy[j] = &#34;&lt;tail&gt; &#34; + tokens_copy[j] + &#34; &lt;/tail&gt;&#34;
                sequence_list.append(&#34; &#34;.join(tokens_copy))
    return sequence_list</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.common.utils.Const"><code class="flex name class">
<span>class <span class="ident">Const</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Const:
    MODEL_NAME = &#34;roberta-base&#34;
    MAX_TOKEN_LEN = 64
    SPECIAL_TOKENS = [
        &#34;&lt;head&gt;&#34;,
        &#34;&lt;/head&gt;&#34;,
        &#34;&lt;tail&gt;&#34;,
        &#34;&lt;/tail&gt;&#34;,
        &#34;&lt;url&gt;&#34;,
        &#34;&lt;user&gt;&#34;,
        &#34;&lt;date&gt;&#34;,
        &#34;&lt;number&gt;&#34;,
        &#34;&lt;money&gt;&#34;,
        &#34;&lt;email&gt;&#34;,
        &#34;&lt;percent&gt;&#34;,
        &#34;&lt;phone&gt;&#34;,
        &#34;&lt;time&gt;&#34;,
        &#34;&lt;hashtag&gt;&#34;,
        &#34;&lt;/hashtag&gt;&#34;,
    ]
    NORMALIZE = [
        &#34;url&#34;,
        &#34;email&#34;,
        &#34;percent&#34;,
        &#34;money&#34;,
        &#34;phone&#34;,
        &#34;user&#34;,
        &#34;time&#34;,
        &#34;url&#34;,
        &#34;date&#34;,
        &#34;number&#34;,
    ]

    TEXT_PROCESSOR_ARGS = dict(
        normalize=NORMALIZE,
        annotate={&#34;hashtag&#34;},
        fix_html=True,
        segmenter=&#34;twitter&#34;,
        corrector=&#34;twitter&#34;,
        unpack_hashtags=True,
        unpack_contractions=True,
        spell_correct_elong=False,
        tokenizer=SocialTokenizer(lowercase=True).tokenize,
        dicts=[emoticons],
    )</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="src.common.utils.Const.MAX_TOKEN_LEN"><code class="name">var <span class="ident">MAX_TOKEN_LEN</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.common.utils.Const.MODEL_NAME"><code class="name">var <span class="ident">MODEL_NAME</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.common.utils.Const.NORMALIZE"><code class="name">var <span class="ident">NORMALIZE</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.common.utils.Const.SPECIAL_TOKENS"><code class="name">var <span class="ident">SPECIAL_TOKENS</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="src.common.utils.Const.TEXT_PROCESSOR_ARGS"><code class="name">var <span class="ident">TEXT_PROCESSOR_ARGS</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="src.common.utils.Label"><code class="flex name class">
<span>class <span class="ident">Label</span></span>
<span>(</span><span>name: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Class used to create labels based on task.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of task, either GER or REL.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Label:
    def __init__(self, name: str):
        &#34;&#34;&#34;
        Class used to create labels based on task.

        Parameters
        ----------
        name : str
            Name of task, either GER or REL.
        &#34;&#34;&#34;
        self.name = name
        assert self.name in {&#34;GER&#34;, &#34;REL&#34;}, &#34;Type must be either GER or REL&#34;

        if self.name == &#34;GER&#34;:
            self.labels: dict[str, int] = {
                &#34;O&#34;: 0,
                &#34;B-location&#34;: 1,
                &#34;I-location&#34;: 2,
                &#34;L-location&#34;: 3,
                &#34;U-location&#34;: 4,
            }
        elif self.name == &#34;REL&#34;:
            self.labels: dict[str, int] = {
                &#34;contains&#34;: 0,
                &#34;none&#34;: 1,
            }

        self.idx: dict[int, str] = {v: k for k, v in self.labels.items()}
        self.count: int = len(self.labels)</code></pre>
</details>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.common" href="index.html">src.common</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="src.common.utils.combine_biluo" href="#src.common.utils.combine_biluo">combine_biluo</a></code></li>
<li><code><a title="src.common.utils.combine_subwords" href="#src.common.utils.combine_subwords">combine_subwords</a></code></li>
<li><code><a title="src.common.utils.convert_input" href="#src.common.utils.convert_input">convert_input</a></code></li>
<li><code><a title="src.common.utils.encode_labels" href="#src.common.utils.encode_labels">encode_labels</a></code></li>
<li><code><a title="src.common.utils.ents_to_relations" href="#src.common.utils.ents_to_relations">ents_to_relations</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.common.utils.Const" href="#src.common.utils.Const">Const</a></code></h4>
<ul class="">
<li><code><a title="src.common.utils.Const.MAX_TOKEN_LEN" href="#src.common.utils.Const.MAX_TOKEN_LEN">MAX_TOKEN_LEN</a></code></li>
<li><code><a title="src.common.utils.Const.MODEL_NAME" href="#src.common.utils.Const.MODEL_NAME">MODEL_NAME</a></code></li>
<li><code><a title="src.common.utils.Const.NORMALIZE" href="#src.common.utils.Const.NORMALIZE">NORMALIZE</a></code></li>
<li><code><a title="src.common.utils.Const.SPECIAL_TOKENS" href="#src.common.utils.Const.SPECIAL_TOKENS">SPECIAL_TOKENS</a></code></li>
<li><code><a title="src.common.utils.Const.TEXT_PROCESSOR_ARGS" href="#src.common.utils.Const.TEXT_PROCESSOR_ARGS">TEXT_PROCESSOR_ARGS</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.common.utils.Label" href="#src.common.utils.Label">Label</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>